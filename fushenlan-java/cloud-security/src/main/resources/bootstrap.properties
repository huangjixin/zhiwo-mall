spring.logging.path=/apps/logs/

#logging
logging.config=classpath:logback-spring.xml
logging.path=${spring.logging.path}

#server
spring.http.encoding.force=true

#monitor
spring.boot.admin.client.prefer-ip=true
management.security.enabled=false

#batch config  
#spring.batch.job.names = importFileJob  
spring.batch.job.enabled = false
spring.batch.initializer.enabled=false

#DB
spring.datasource.driver-class-name=com.mysql.jdbc.Driver

#Tomcat JDBC Pool
spring.datasource.tomcat.default-auto-commit=true
spring.datasource.tomcat.initial-size=10
spring.datasource.tomcat.max-active=120
spring.datasource.tomcat.max-wait=10000
#取出一个连接时是否进行验证，若验证失败则从池中删除该连接并尝试取出另一个连接
spring.datasource.tomcat.test-on-borrow=true
#对池中空闲的连接是否进行验证，验证失败则回收此连接
spring.datasource.tomcat.test-while-idle=true
#spring.datasource.tomcat.validation-query=SELECT 1
spring.datasource.tomcat.validation-query-timeout=3
#决定线程多久验证空闲连接或丢弃连接的频率
spring.datasource.tomcat.time-between-eviction-runs-millis=10000
#连接在池中保持空闲而不被回收的最小时间
spring.datasource.tomcat.min-evictable-idle-time-millis=120000
#标记是否删除泄露的连接，如果连接超出removeAbandonedTimeout的限制，则连接被认为是被泄露并且可以被删除
spring.datasource.tomcat.remove-abandoned=true
spring.datasource.tomcat.remove-abandoned-timeout=60000


#multipart
spring.http.multipart.max-file-size=100MB
spring.http.multipart.max-request-size=100MB

#spring.kafka.bootstrap-servers=${KAFKA_SERVERS}

#spring.kafka.consumer.group-id=group
#spring.kafka.consumer.auto-offset-reset=earliest
#spring.kafka.consumer.enable-auto-commit=false
#spring.kafka.consumer.max-poll-records=10
#spring.kafka.consumer.key-deserializer=org.apache.kafka.common.serialization.StringDeserializer
#spring.kafka.consumer.value-deserializer=com.fulan.core.monitoring.log.gather.kafka.serialization.BeanDeserializer
#spring.kafka.listener.concurrency=3
#spring.kafka.listener.ack-mode=manual

#spring.kafka.producer.acks=all
#spring.kafka.producer.batch-size=10485760
#spring.kafka.producer.buffer-memory=33554432
#spring.kafka.producer.retries=1
#spring.kafka.producer.key-serializer=org.apache.kafka.common.serialization.StringSerializer
#spring.kafka.producer.value-serializer=com.fulan.core.monitoring.log.gather.kafka.serialization.BeanSerializer

#kafka.additional.producer.max.block.ms=5000
#kafka.appcodes=user
#kafka.topic=acmplog
#kafka.tail-log=log
#kafka.tail-tlog=tlog

spring.kafka.bootstrap-servers=10.186.124.61:9092,10.186.124.239:9092,10.186.124.205:9092

server.port=31005
eureka.client.service-url.defaultZone=http://127.0.0.1:31000/eureka/
#mybatis
mybatis-plus.mapperLocations=classpath:mybatis/*.xml
mybatis-plus.globalConfig.metaObjectHandler=com.fulan.application.orm.audit.AuditMetaObjectHandler
mybatis-plus.globalConfig.dbColumnUnderline=true

#config setting
spring.cloud.config.discovery.enabled=true
spring.cloud.config.discovery.serviceId=config
spring.cloud.config.profile=sit
#spring.cloud.config.profile=dev
spring.cloud.config.name=security